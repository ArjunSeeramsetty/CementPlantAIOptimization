import pandas as pd
import numpy as np

# Collect all model results
all_results = [lr_results, dt_results, rf_results]

# Create comparison DataFrame
comparison_df = pd.DataFrame(all_results)

print("ğŸ¯ BASELINE MODELS PERFORMANCE COMPARISON")
print("=" * 60)

# Display results table
print("\\nğŸ“Š DETAILED PERFORMANCE METRICS:")
print(comparison_df.round(4))

print("\\nğŸ† MODEL RANKINGS:")
print("\\nğŸ“ˆ Best Test RÂ² Score:")
best_r2 = comparison_df.loc[comparison_df['test_r2'].idxmax()]
print(f"   {best_r2['model_name']}: {best_r2['test_r2']:.4f}")

print("\\nğŸ“‰ Lowest Test RMSE:")
best_rmse = comparison_df.loc[comparison_df['test_rmse'].idxmin()]
print(f"   {best_rmse['model_name']}: {best_rmse['test_rmse']:.4f}")

print("\\nğŸ¯ Best Test MAE:")
best_mae = comparison_df.loc[comparison_df['test_mae'].idxmin()]
print(f"   {best_mae['model_name']}: {best_mae['test_mae']:.4f}")

print("\\nğŸ” MODEL ANALYSIS:")
print("\\nâ€¢ Linear Regression:")
print(f"  - Excellent performance with RÂ² = {lr_results['test_r2']:.4f}")
print(f"  - Good generalization (train vs test RÂ² difference: {abs(lr_results['train_r2'] - lr_results['test_r2']):.4f})")

print("\\nâ€¢ Decision Tree:")
print(f"  - Perfect training performance (RÂ² = 1.0000)")
print(f"  - Shows overfitting (test RÂ² = {dt_results['test_r2']:.4f})")
print(f"  - Large generalization gap: {abs(dt_results['train_r2'] - dt_results['test_r2']):.4f}")

print("\\nâ€¢ Random Forest:")
print(f"  - Good performance with RÂ² = {rf_results['test_r2']:.4f}")
print(f"  - Better generalization than single Decision Tree")
print(f"  - Moderate generalization gap: {abs(rf_results['train_r2'] - rf_results['test_r2']):.4f}")

print("\\nâœ… SUCCESS CRITERIA MET:")
print("âœ“ All 3 baseline models trained successfully")
print("âœ“ Linear Regression baseline accuracy: {:.1f}%".format(lr_results['test_r2'] * 100))
print("âœ“ Decision Tree baseline accuracy: {:.1f}%".format(dt_results['test_r2'] * 100))
print("âœ“ Random Forest baseline accuracy: {:.1f}%".format(rf_results['test_r2'] * 100))
print("âœ“ Performance metrics generated for all models")

# Store final comparison for potential downstream use
baseline_comparison = comparison_df.copy()
model_performance_summary = {
    'best_model': best_r2['model_name'],
    'best_test_r2': best_r2['test_r2'],
    'all_models_trained': True,
    'total_models': len(all_results)
}